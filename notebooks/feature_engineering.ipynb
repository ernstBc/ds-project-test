{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d09b134",
   "metadata": {},
   "source": [
    "## Data Transformation Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d3c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ernes\\Documents\\ML Projects\\ds-project-test\\ds-project-test\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d11df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a6a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"Data Transformation Configuration\"\"\"\n",
    "    data_dir : Path\n",
    "    training_csv_dir : Path\n",
    "    testing_csv_dir : Path\n",
    "    preprocessor_dir : Path\n",
    "    params: Dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dcb30e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 13:28:18 - INFO: __init__ - Logging setup complete.\n"
     ]
    }
   ],
   "source": [
    "from src.ds_project.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from src.ds_project.utils.utils import read_yaml, create_directories\n",
    "\n",
    "\n",
    "class DataTransformationManager:\n",
    "    def __init__(self, config_path: str = CONFIG_FILE_PATH, params_path: str = PARAMS_FILE_PATH) -> DataTransformationConfig:\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "\n",
    "    def create_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = DataTransformationConfig(\n",
    "            data_dir=self.config['data_ingestion']['local_data_file'],\n",
    "            training_csv_dir=self.config['data_transformation']['training_csv_dir'],\n",
    "            testing_csv_dir=self.config['data_transformation']['testing_csv_dir'],\n",
    "            preprocessor_dir=self.config['data_transformation']['preprocessor_file_path'],\n",
    "            params=self.params['transform_data']\n",
    "        )\n",
    "        create_directories([self.config['data_transformation']['directory']])\n",
    "        create_directories([self.config['data_transformation']['transformation_artifacts']])\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ds_project import logger\n",
    "from src.ds_project.utils.utils import save_binary_data\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def save_data(data: pd.DataFrame, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the transformed data to a CSV file.\n",
    "    data (Dataframe): A pandas Dataframe \n",
    "    file_path(str): The file directory where the data will be store \n",
    "                    (must include a .csv at tne end of the path name)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data.to_csv(file_path, index=False)\n",
    "        logger.info(f\"Data saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data: {e}\")\n",
    "        raise e\n",
    "\n",
    "def feature_selection(data: pd.DataFrame, features_to_drop: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select relevant features from the dataset dropping irrelevant features.\n",
    "    Arguments:\n",
    "        data (Dataframe): A pandas DataFrame\n",
    "        features_to_drop(list): A list with the name of the columns to drop\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame without the dropped columns\n",
    "    \"\"\"\n",
    "    data = data.drop(columns=features_to_drop, errors='ignore')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_prepocessor(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Creates a preprocessor object where the needed data transformations are applied.\n",
    "    Arguments:\n",
    "    Arguments:\n",
    "        data(DataFrame): A pandas DataFrame\n",
    "    Returns:\n",
    "        preprocessor(ColumnTranform): A sklearn.ColumnTransform object\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor.fit(data)\n",
    "    logger.info(\"Preprocessor created successfully.\")\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load data from the specified directory.\"\"\"\n",
    "        try:\n",
    "            data = pd.read_csv(self.config.data_dir)\n",
    "            logger.info(\"Data loaded successfully.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise e\n",
    "    \n",
    "\n",
    "    def transform(self) -> None:\n",
    "        \"\"\"Transform data by applying preprocessing steps.\"\"\"\n",
    "        data = self.load_data()\n",
    "        try:\n",
    "            # Feature selection\n",
    "            data = feature_selection(data, features_to_drop=self.config.params['DROPPED_COLUMNS'])\n",
    "\n",
    "            # Split data into training and testing sets\n",
    "            training_data, testing_data = train_test_split(data, \n",
    "                                                           test_size=self.config.params['TEST_SIZE'],\n",
    "                                                           random_state=self.config.params['RANDOM_SEED'])\n",
    "\n",
    "            # Create preprocessor\n",
    "            preprocessor = create_prepocessor(training_data.drop(columns=['Survived']))\n",
    "\n",
    "            # Transform data using the preprocessor\n",
    "            transformed_training = preprocessor.transform(training_data.drop(columns=['Survived']))\n",
    "            transformed_training = pd.DataFrame(transformed_training, columns=preprocessor.get_feature_names_out())\n",
    "            transformed_training['Survived'] = training_data['Survived'].values\n",
    "\n",
    "            transformed_testing = preprocessor.transform(testing_data.drop(columns=['Survived']))\n",
    "            transformed_testing = pd.DataFrame(transformed_testing, columns=preprocessor.get_feature_names_out())\n",
    "            transformed_testing['Survived'] = testing_data['Survived'].values\n",
    "\n",
    "            logger.info(\"Data transformation completed successfully.\")\n",
    "\n",
    "            # Save transformed data\n",
    "            save_data(transformed_training, self.config.training_csv_dir)\n",
    "            save_data(transformed_testing, self.config.testing_csv_dir)\n",
    "\n",
    "            # Save preprocessor\n",
    "            save_binary_data(Path(self.config.preprocessor_dir), preprocessor, as_pickle=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data transformation: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8280f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 13:28:20 - INFO: utils - YAML file config\\config.yaml loaded successfully.\n",
      "2025-05-03 13:28:20 - INFO: utils - YAML file parameters.yaml loaded successfully.\n",
      "2025-05-03 13:28:20 - INFO: utils - Directory artifact/data_transformation created.\n",
      "2025-05-03 13:28:20 - INFO: utils - Directory artifact/data_transformation/artifacts created.\n",
      "2025-05-03 13:28:21 - INFO: 4110887165 - Data loaded successfully.\n",
      "2025-05-03 13:28:21 - INFO: 4110887165 - Preprocessor created successfully.\n",
      "2025-05-03 13:28:21 - INFO: 4110887165 - Data transformation completed successfully.\n",
      "2025-05-03 13:28:21 - INFO: 4110887165 - Data saved to artifact/data_ingestion/train_data/train.csv\n",
      "2025-05-03 13:28:21 - INFO: 4110887165 - Data saved to artifact/data_ingestion/test_data/test.csv\n",
      "2025-05-03 13:28:21 - INFO: utils - Binary file artifact\\data_transformation\\artifacts\\preprocessing.pkl saved successfully.\n"
     ]
    }
   ],
   "source": [
    "manager= DataTransformationManager()\n",
    "config = manager.create_data_transformation_config()\n",
    "transformer= DataTransformation(config=config)\n",
    "transformer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "282dd5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifact/data_transformation/artifacts/preprocessing.pkl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.preprocessor_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a46423b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtransformed_data\u001b[49m[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'transformed_data' is not defined"
     ]
    }
   ],
   "source": [
    "transformed_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f149c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 23:51:45 - INFO: utils - YAML file parameters.yaml loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConfigBox({'transform_data': {'RANDOM_SEED': 86, 'TEST_SIZE': 0.2, 'CATEGORICAL_COLUMNS': ['Pclass'], 'NUMERICAL_COLUMNS': ['Age', 'Fare'], 'DROPPED_COLUMNS': ['PassengerId', 'Name', 'Ticket', 'Cabin']}, 'training': {'TARGET_COLUMN': 'Survived', 'TEST_SIZE': 0.2, 'RANDOM_SEED': 42}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_yaml(PARAMS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0a464",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
