{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfad263f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ernes\\Documents\\ML Projects\\ds-project-test\\ds-project-test\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8779f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelValidatorConfig:\n",
    "    model_path: Path\n",
    "    model_name:str\n",
    "    model_type:str\n",
    "    test_data_path: Path\n",
    "    best_model_path: Path\n",
    "    best_model_artifacts: Path\n",
    "    model_registry_path: Path\n",
    "    params: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f79f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 13:24:54 - INFO: __init__ - Logging setup complete.\n"
     ]
    }
   ],
   "source": [
    "from src.ds_project.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from src.ds_project.utils.utils import read_yaml, create_directories\n",
    "import os\n",
    "\n",
    "class ModelValidatorManager:\n",
    "    def __init__(self,\n",
    "                 config_path: str = CONFIG_FILE_PATH,\n",
    "                 params_path: str = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config= read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        create_directories([self.config.model_validator['directory'], \n",
    "                            self.config.model_validator['best_model_path'],\n",
    "                            self.config.model_validator['best_model_artifacts_path']])\n",
    "        \n",
    "    def get_model_validator_config(self, model_name:str, model_type:str):\n",
    "        model_path = os.path.join(self.config.model_trainer['directory'], model_name)\n",
    "        config=ModelValidatorConfig(\n",
    "            model_path=Path(model_path),\n",
    "            model_name=model_name,\n",
    "            model_type=model_type,\n",
    "            test_data_path=self.config.model_trainer['testing_csv_dir'],\n",
    "            best_model_path=self.config.model_validator['best_model_path'],\n",
    "            best_model_artifacts=self.config.model_validator['best_model_artifacts_path'],\n",
    "            model_registry_path=self.config.model_validator['model_registry'],\n",
    "            params=self.params\n",
    "        )\n",
    "\n",
    "        return config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5c542ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from src.ds_project import logger\n",
    "from src.ds_project.constants import VALIDATOR_THRESHOLD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (f1_score, \n",
    "                             accuracy_score,\n",
    "                             recall_score,\n",
    "                             precision_score,\n",
    "                             classification_report, \n",
    "                             ConfusionMatrixDisplay, \n",
    "                             RocCurveDisplay,\n",
    "                             PrecisionRecallDisplay)\n",
    "from src.ds_project.utils.utils import (load_binary_data, \n",
    "                                        save_json,\n",
    "                                        save_binary_data)\n",
    "\n",
    "\n",
    "class ModelValidator:\n",
    "    def __init__(self, config: ModelValidatorConfig, threshold:float = VALIDATOR_THRESHOLD):\n",
    "        self.config=config\n",
    "        self.threshold=threshold\n",
    "\n",
    "\n",
    "    def validate_model(self):\n",
    "\n",
    "        target_column = self.config.params.training['TARGET_COLUMN']\n",
    "        model_dir = Path(os.path.join(self.config.model_path, 'model.pkl'))\n",
    "        model = load_binary_data(model_dir)\n",
    "        old_model_path=Path(os.path.join(self.config.best_model_path, 'model.pkl'))\n",
    "\n",
    "        # load data\n",
    "        data_test = pd.read_csv(self.config.test_data_path)\n",
    "        x_test, y_test = data_test.drop([target_column], axis=1), data_test[target_column]\n",
    "\n",
    "        if 'model.pkl' not in os.listdir(self.config.best_model_path):\n",
    "            logger.info(f'Current model is the best model')\n",
    "            \n",
    "            # save the model as the best model\n",
    "            save_binary_data(old_model_path, data=model, as_pickle=True)\n",
    "\n",
    "            # calculate predictions and get metrics\n",
    "            preds = model.predict(x_test)\n",
    "\n",
    "            # log metrics and registry the model\n",
    "            metrics=save_metrics_image(y_test, preds, self.config.best_model_artifacts)\n",
    "            register_model(self.config.model_registry_path, \n",
    "                           model_name=self.config.model_name,\n",
    "                           model_type=self.config.model_type,\n",
    "                           metrics=metrics,\n",
    "                           params=self.config.params.training.MODELS[self.config.model_type])\n",
    "\n",
    "        else:\n",
    "            old_model=load_binary_data(old_model_path)\n",
    "            old_preds=old_model.predict(x_test)\n",
    "            \n",
    "            new_preds=model.predict(x_test)\n",
    "\n",
    "            f1_old = f1_score(y_test, old_preds)\n",
    "            f1_new = f1_score(y_test, new_preds)\n",
    "\n",
    "            if (f1_new) > (f1_old + self.threshold):\n",
    "\n",
    "                logger.info(f'Current model replace the lastest best model')\n",
    "                # save the model as the best model\n",
    "                save_binary_data(old_model_path,\n",
    "                                data=model,\n",
    "                                as_pickle=True)\n",
    "                # log metrics and model\n",
    "                metrics=save_metrics_image(y_test, preds, self.config.best_model_artifacts)\n",
    "                register_model(self.config.model_registry_path, \n",
    "                                model_name=self.config.model_name,\n",
    "                                model_type=self.config.model_type,\n",
    "                                metrics=metrics,\n",
    "                                params=self.config.params.training.MODELS[self.config.model_type])\n",
    "            else:\n",
    "                logger.info(\"Model doesn't beat the current best model\")\n",
    "            \n",
    "\n",
    "\n",
    "def save_metrics_image(y_real: pd.Series, y_pred:pd.Series, path_dir:str):\n",
    "    # report\n",
    "    report = classification_report(y_real, y_pred, output_dict=True)\n",
    "    report_path = Path(os.path.join(path_dir, 'report.json'))\n",
    "    save_json(report_path, report)\n",
    "\n",
    "    # plots \n",
    "    cm = ConfusionMatrixDisplay.from_predictions(y_real, y_pred)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(path_dir, 'confusion_matrix.png'))    \n",
    "    plt.clf()\n",
    "\n",
    "    roc = RocCurveDisplay.from_predictions(y_real, y_pred)\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(os.path.join(path_dir,'roc_curve.png'))\n",
    "    plt.clf()\n",
    "\n",
    "    rp_curve=PrecisionRecallDisplay.from_predictions(y_real, y_pred)\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.savefig(os.path.join(path_dir,'precision_recall.png'))    \n",
    "    plt.clf()      \n",
    "\n",
    "    accuracy = accuracy_score(y_real, y_pred)\n",
    "    f1 = f1_score(y_real, y_pred)\n",
    "    precision=precision_score(y_real, y_pred)\n",
    "    recall = recall_score(y_real, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy':accuracy,\n",
    "        'f1':f1,\n",
    "        \"precision\":precision,\n",
    "        \"recall\":recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def register_model(model_registory_path:str, \n",
    "                   model_name:str, \n",
    "                   model_type:str,\n",
    "                   metrics: dict, \n",
    "                   params:dict):\n",
    "    \n",
    "    format_text = f\"\"\"\n",
    "    {'--'*150}\n",
    "    Model: {model_name}\n",
    "        Type: {model_type}\n",
    "            Metrics:\n",
    "                {metrics}\n",
    "            Params: \n",
    "                {params}\n",
    "    {'--'*150}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    with open(model_registory_path, 'a') as file:\n",
    "        file.write(format_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9a6fd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 13:51:54 - INFO: utils - YAML file config\\config.yaml loaded successfully.\n",
      "2025-05-11 13:51:54 - INFO: utils - YAML file parameters.yaml loaded successfully.\n",
      "2025-05-11 13:51:54 - INFO: utils - Directory artifact/model_validator created.\n",
      "2025-05-11 13:51:54 - INFO: utils - Directory artifact/model_validator/best_model created.\n",
      "2025-05-11 13:51:54 - INFO: utils - Directory artifact/model_validator/best_model/artifacts created.\n",
      "2025-05-11 13:51:54 - INFO: utils - Binary file artifact\\model_trainer\\2025-05-05-17-49-04\\model.pkl loaded successfully.\n",
      "2025-05-11 13:51:54 - INFO: utils - Binary file artifact\\model_validator\\best_model\\model.pkl loaded successfully.\n",
      "2025-05-11 13:51:54 - INFO: 308188043 - Model doesn't beat the current best model\n"
     ]
    }
   ],
   "source": [
    "config=ModelValidatorManager(r'artifact\\model_trainer\\2025-05-05-17-49-04')\n",
    "config_p = config.get_model_validator_config(model_name='2025-05-05-17-49-04', model_type='LOGISTIC_REGRESSION')\n",
    "\n",
    "validator = ModelValidator(config_p)\n",
    "\n",
    "validator.validate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "549d2d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfigBox({'penalty': 'l2', 'C': 0.8, 'max_iter': 100, 'random_state': 86})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_p.params.training.MODELS['LOGISTIC_REGRESSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69452f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
